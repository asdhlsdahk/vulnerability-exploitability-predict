from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM, BertConfig
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import pandas as pd
import csv
import os
# ——————构造模型——————

savebert = "E:\大学学习\研究生\毕设\\BERT\\onlybert"

class TextNet(nn.Module):
    def __init__(self, code_length):  # code_length为fc映射到的维度大小
        super(TextNet, self).__init__()

        modelConfig = BertConfig.from_pretrained('bert-base-uncased-config.json')
        self.textExtractor = BertModel.from_pretrained(
            'bert-base-uncased-pytorch_model.bin', config=modelConfig)
        embedding_dim = self.textExtractor.config.hidden_size

        self.fc = nn.Linear(embedding_dim, code_length)
        self.tanh = torch.nn.Tanh()

    def forward(self, tokens, segments, input_masks):
        output = self.textExtractor(tokens, token_type_ids=segments,
                                    attention_mask=input_masks)
        text_embeddings = output[0][:, 0, :]
        # output[0](batch size, sequence length, model hidden dimension)

        features = self.fc(text_embeddings)
        features = self.tanh(features)
        return features

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased-vocab.txt')
dirname = "E:\大学学习\大学学习\大三下\信安赛\数据源\结果合成\\fenju"

for filename in os.listdir(dirname):
    csvname = dirname + "\\" + filename
    targetname = savebert + "\\" + filename
    texts = []

    data = pd.read_csv(csvname)
    des = data["DESCR"][0]
    texts.append(des)
    textNet = TextNet(code_length=64)
    tokens, segments, input_masks = [], [], []
    for text in texts:
        tokenized_text = tokenizer.tokenize(text)  # 用tokenizer对句子分词
        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)  # 索引列表
        tokens.append(indexed_tokens)
        segments.append([0] * len(indexed_tokens))
        input_masks.append([1] * len(indexed_tokens))

    max_len = max([len(single) for single in tokens])  # 最大的句子长度

    for j in range(len(tokens)):
        padding = [0] * (max_len - len(tokens[j]))
        tokens[j] += padding
        segments[j] += padding
        input_masks[j] += padding
    # segments列表全0，因为只有一个句子1，没有句子2
    # input_masks列表1的部分代表句子单词，而后面0的部分代表paddig，只是用于保持输入整齐，没有实际意义。
    # 相当于告诉BertModel不要利用后面0的部分
    tokens_tensor = torch.tensor(tokens)
    segments_tensors = torch.tensor(segments)
    input_masks_tensors = torch.tensor(input_masks)
    # ——————提取文本特征——————
    text_hashCodes = textNet(tokens_tensor, segments_tensors, input_masks_tensors)  # text_hashCodes是一个32-dim文本特征
    # a = text_hashCodes[0].tolist()
    # print(text_hashCodes)
    # print(a)
    # print(text_hashCodes.detach().numpy())
    # print(type(text_hashCodes.detach().numpy()))
    np.savetxt(targetname, text_hashCodes.detach().numpy(), delimiter=' ')

